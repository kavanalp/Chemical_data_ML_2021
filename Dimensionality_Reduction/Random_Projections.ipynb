{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random Projections.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNWIqKkZ0vAe1eQ+ZuG/qWL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/panahiparham/mlclass_proj_f2021/blob/main/Dimensionality_Reduction/Random_Projections.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Projection\n",
        "\n",
        "Random Projection is an efficient method for reducing minesionality of data\n",
        "\n",
        "the tradeoff in this method is loosing some accuracy for working in low dimensions.\n",
        "\n",
        "This method is not as accurate as PCA in finding best subspace but is quite efficient\n",
        "\n",
        "\n",
        "https://scikit-learn.org/stable/modules/random_projection.html"
      ],
      "metadata": {
        "id": "Etw_RM5Tn_t5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Johnson-Lindenstrauss lemma\n",
        "\n",
        "Let $Q$ be a finite set of vectors in $R^d$. Let $\\delta \\in (0,1)$ and $n$ be an interger such that \n",
        "$$\n",
        "ϵ = \\sqrt{\\frac{6 \\log(2|Q|/δ)}{n}}\n",
        "$$\n",
        "Then with probability of at least $1-δ$ over a choice of a random matrix $W\\in R^{n,d}$ such that each element of $W$ is distributed normally with zero mean and variance of $1/n$ we have\n",
        "$$\n",
        "\\sup_{x\\in Q}{|\\frac{||Wx||^2}{||x||^2} -1|}<ϵ.\n",
        "$$\n",
        "\n",
        "\n",
        "<em>ref: S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From\n",
        "Theory to Algorithms. Cambridge University Press, 2014\n",
        "</em>\n",
        "\n",
        "Given number of samples and epsilon we can find the minimal required $n$ for JL lemma to hold."
      ],
      "metadata": {
        "id": "9PVxkux1r0rJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
        "johnson_lindenstrauss_min_dim(n_samples = 134000, eps=[0.5, 0.3, 0.2, 0.1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "og1L6quwu7GZ",
        "outputId": "ff7c473b-d84f-45bb-a2f5-df769117572b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  566,  1311,  2724, 10119])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our original dimension is less than 200 therefore, random projections is not applicable to our data!"
      ],
      "metadata": {
        "id": "b91qCNOEybPe"
      }
    }
  ]
}